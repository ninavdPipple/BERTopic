{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninavdPipple/BERTopic/blob/master/Parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk6JXn6KVj8v"
      },
      "source": [
        "# Load packages and arrange right set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3xTZlaIn0xGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ecbb93f-ee4d-4f72-e154-4c2239ce3493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7nO0TehEcdJ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24a5cdb3-8c25-47a4-f0bd-389ddb7836ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (4.9.0)\n",
            "Collecting typing_extensions\n",
            "  Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: typing_extensions\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "Successfully installed typing_extensions-4.10.0\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.16.0-py2.py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.25.2)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.2)\n",
            "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
            "  Downloading sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic)\n",
            "  Using cached Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (23.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.3.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.37.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Downloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.10.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Building wheels for collected packages: hdbscan, umap-learn\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3039298 sha256=08fb5defd5ad19915e152d2072826ce951f2511456072930d4512afe961e863a\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=1ba5516f1338b66f29a27c9823d436faa6ae1588b83c986e5422bf3a165b2497\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n",
            "Successfully built hdbscan umap-learn\n",
            "Installing collected packages: cython, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.8\n",
            "    Uninstalling Cython-3.0.8:\n",
            "      Successfully uninstalled Cython-3.0.8\n",
            "Successfully installed bertopic-0.16.0 cython-0.29.37 hdbscan-0.8.33 pynndescent-0.5.11 sentence-transformers-2.4.0 umap-learn-0.5.5\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m889.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tomotopy\n",
            "  Downloading tomotopy-0.12.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tomotopy) (1.25.2)\n",
            "Installing collected packages: tomotopy\n",
            "Successfully installed tomotopy-0.12.7\n",
            "Collecting octis\n",
            "  Downloading octis-1.13.1-py2.py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim==4.2.0 (from octis)\n",
            "  Downloading gensim-4.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from octis) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from octis) (1.5.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from octis) (3.7.4)\n",
            "Collecting scikit-learn==1.1.0 (from octis)\n",
            "  Downloading scikit_learn-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-optimize>=0.8.1 (from octis)\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from octis) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from octis) (2.1.0+cu121)\n",
            "Collecting numpy==1.23.0 (from octis)\n",
            "  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting libsvm (from octis)\n",
            "  Downloading libsvm-3.23.0.4.tar.gz (170 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.6/170.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from octis) (2.2.5)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from octis) (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from octis) (2.31.0)\n",
            "Requirement already satisfied: tomotopy in /usr/local/lib/python3.10/dist-packages (from octis) (0.12.7)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->octis) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->octis) (6.4.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.0->octis) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.0->octis) (3.3.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize>=0.8.1->octis)\n",
            "  Downloading pyaml-23.12.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (8.1.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (2.8.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->octis) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->octis) (4.66.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->octis) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (2024.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->octis) (4.37.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->octis) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->octis) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->octis) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->octis) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->octis) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->octis) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->octis) (2.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (0.9.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers->octis) (6.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->octis) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->octis) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->octis) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->octis) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->octis) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->octis) (0.1.4)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers->octis) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers->octis) (0.4.2)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->octis) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->octis) (1.3.0)\n",
            "Building wheels for collected packages: libsvm\n",
            "  Building wheel for libsvm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libsvm: filename=libsvm-3.23.0.4-cp310-cp310-linux_x86_64.whl size=251405 sha256=0119f88700e15923c8ac98d7c89176c4f566217ed603bb4fbbbaa784c18f18ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/c7/19/a8c85928f8e629654b8e1adb3c8091f0bb77344d0ee9954a85\n",
            "Successfully built libsvm\n",
            "Installing collected packages: pyaml, numpy, libsvm, scikit-learn, gensim, scikit-optimize, octis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.2\n",
            "    Uninstalling gensim-4.3.2:\n",
            "      Successfully uninstalled gensim-4.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.21.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.0 which is incompatible.\n",
            "chex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.23.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.2.0 libsvm-3.23.0.4 numpy-1.23.0 octis-1.13.1 pyaml-23.12.0 scikit-learn-1.1.0 scikit-optimize-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "sklearn"
                ]
              },
              "id": "7959f134a3e7487fa07255ee48e9589d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from /nbs/Evaluation_BERTopic_repo.ipynb\n",
            "Requirement already satisfied: octis in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: gensim==4.2.0 in /usr/local/lib/python3.10/dist-packages (from octis) (4.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from octis) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from octis) (1.5.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from octis) (3.7.4)\n",
            "Requirement already satisfied: scikit-learn==1.1.0 in /usr/local/lib/python3.10/dist-packages (from octis) (1.1.0)\n",
            "Requirement already satisfied: scikit-optimize>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from octis) (0.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from octis) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from octis) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy==1.23.0 in /usr/local/lib/python3.10/dist-packages (from octis) (1.23.0)\n",
            "Requirement already satisfied: libsvm in /usr/local/lib/python3.10/dist-packages (from octis) (3.23.0.4)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from octis) (2.2.5)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from octis) (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from octis) (2.31.0)\n",
            "Requirement already satisfied: tomotopy in /usr/local/lib/python3.10/dist-packages (from octis) (0.12.7)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->octis) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->octis) (6.4.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.0->octis) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.0->octis) (3.3.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize>=0.8.1->octis) (23.12.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (8.1.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (2.8.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->octis) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->octis) (4.66.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->octis) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (2024.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->octis) (4.37.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->octis) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->octis) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->octis) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->octis) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->octis) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->octis) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->octis) (2.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (0.9.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers->octis) (6.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->octis) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->octis) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->octis) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->octis) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->octis) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->octis) (0.1.4)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers->octis) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers->octis) (0.4.2)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->octis) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->octis) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: octis in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: gensim==4.2.0 in /usr/local/lib/python3.10/dist-packages (from octis) (4.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from octis) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from octis) (1.5.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from octis) (3.7.4)\n",
            "Requirement already satisfied: scikit-learn==1.1.0 in /usr/local/lib/python3.10/dist-packages (from octis) (1.1.0)\n",
            "Requirement already satisfied: scikit-optimize>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from octis) (0.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from octis) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from octis) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy==1.23.0 in /usr/local/lib/python3.10/dist-packages (from octis) (1.23.0)\n",
            "Requirement already satisfied: libsvm in /usr/local/lib/python3.10/dist-packages (from octis) (3.23.0.4)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from octis) (2.2.5)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from octis) (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from octis) (2.31.0)\n",
            "Requirement already satisfied: tomotopy in /usr/local/lib/python3.10/dist-packages (from octis) (0.12.7)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->octis) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->octis) (6.4.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.0->octis) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.0->octis) (3.3.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize>=0.8.1->octis) (23.12.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (8.1.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (2.8.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->octis) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->octis) (4.66.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->octis) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (2024.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->octis) (4.37.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->octis) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->octis) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->octis) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->octis) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->octis) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->octis) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->octis) (2.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (0.9.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers->octis) (6.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->octis) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->octis) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->octis) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->octis) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->octis) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->octis) (0.1.4)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers->octis) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers->octis) (0.4.2)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->octis) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->octis) (1.3.0)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.23.0)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.33)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.5)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.1.0)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.4.0)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.37)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (23.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.3.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.37.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.10.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install typing_extensions --upgrade\n",
        "!pip install bertopic\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from joblib import Memory\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "!pip install kora -q\n",
        "from kora import drive\n",
        "drive.link_nbs()\n",
        "!python -m pip install tomotopy\n",
        "!pip install octis\n",
        "from octis.dataset.dataset import Dataset\n",
        "from Evaluation_BERTopic_repo import Trainer, DataLoader, Evaluator\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "!pip install openai\n",
        "import openai\n",
        "from openai import AzureOpenAI\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', -1) # make sure to print entire messages when printed form dataframe\n",
        "import regex as re\n",
        "import statsmodels.stats.api as sms\n",
        "from sklearn.metrics import f1_score\n",
        "from csv import DictWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NESIY-SihtJa"
      },
      "outputs": [],
      "source": [
        "# Connecting to GPT model\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_key=\"139ff4ea58234550972e6f5db98b30a7\",\n",
        "    api_version='2023-05-15',\n",
        "    azure_endpoint = \"https://510-openai.openai.azure.com/\"\n",
        ")\n",
        "\n",
        "deployment_name='510-chat' #This will correspond to the custom name you chose for your deployment when you deployed a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQW-vuqTFcCC"
      },
      "source": [
        "# Define functions for training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L5ocSpO4lyDZ"
      },
      "outputs": [],
      "source": [
        "def remove_emojis(text):\n",
        "  # Define regex expression for emojis\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "  # Remove emojis\n",
        "  modified_text = emoji_pattern.sub(r'.', text)\n",
        "  modified_text = re.sub(r'([\\:\\<]-?[)(|\\\\/pP3D])(?:(?=\\s))', '.', modified_text)\n",
        "  modified_text = modified_text.lstrip('.') # Remove dots at start of message that arise as a result\n",
        "\n",
        "  return modified_text\n",
        "\n",
        "# Remove sequential interpunction\n",
        "def remove_repeats(text):\n",
        "  modified_text=re.sub(r'([^\\w])\\s*([^\\w\\s])+', lambda match: '? ' if '?' in match.group() else match.group(2), text)\n",
        "\n",
        "  return modified_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r1OyDowgBtU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dfe2ee8-b753-4070-dc72-d7261cf9a60e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from /nbs/OCTIS_preprocess_repo.ipynb\n",
            "Requirement already satisfied: octis in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: gensim==4.2.0 in /usr/local/lib/python3.10/dist-packages (from octis) (4.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from octis) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from octis) (1.5.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from octis) (3.7.4)\n",
            "Requirement already satisfied: scikit-learn==1.1.0 in /usr/local/lib/python3.10/dist-packages (from octis) (1.1.0)\n",
            "Requirement already satisfied: scikit-optimize>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from octis) (0.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from octis) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from octis) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy==1.23.0 in /usr/local/lib/python3.10/dist-packages (from octis) (1.23.0)\n",
            "Requirement already satisfied: libsvm in /usr/local/lib/python3.10/dist-packages (from octis) (3.23.0.4)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from octis) (2.2.5)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from octis) (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from octis) (2.31.0)\n",
            "Requirement already satisfied: tomotopy in /usr/local/lib/python3.10/dist-packages (from octis) (0.12.7)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->octis) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0->octis) (6.4.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.0->octis) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.0->octis) (3.3.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize>=0.8.1->octis) (23.12.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->octis) (8.1.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->octis) (2.8.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->octis) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->octis) (4.66.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->octis) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->octis) (2024.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->octis) (4.37.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->octis) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->octis) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->octis) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->octis) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->octis) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->octis) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->octis) (2.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (0.9.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (2.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->octis) (3.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers->octis) (6.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->octis) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->octis) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->octis) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->octis) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->octis) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->octis) (0.1.4)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers->octis) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers->octis) (0.4.2)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy->octis) (0.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->octis) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from OCTIS_preprocess_repo import Preprocessing\n",
        "os.chdir(os.path.pardir)\n",
        "\n",
        "def pre_process_data(raw_data, pre_process, reformulations_path = None):\n",
        "\n",
        "  # Distinguish between reformulations and normal messages to not overwrite data\n",
        "  if reformulations_path:\n",
        "    path = reformulations_path\n",
        "  else:\n",
        "    path = f'/content/drive/MyDrive/2024'\n",
        "\n",
        "  # Pre-process data if still needed\n",
        "  # If using reformulations always pre-process again\n",
        "  if  os.path.exists(f'{path}/ukraine_{pre_process}'): #reformulations_path is None and\n",
        "    # Obtain existing pre-processed dataset\n",
        "    dataset = Dataset('ukraine')\n",
        "    dataset.load_custom_dataset_from_folder(f'{path}/ukraine_{pre_process}')\n",
        "  else:\n",
        "    # Save required data in required format\n",
        "    raw_data['low'] =  raw_data['low'].apply(lambda x: re.sub(r'\\n',' ', str(x)))     # Remove enters within message to process messages correctly\n",
        "    raw_data['low'].to_csv(f'{path}/documents_stripped_low.csv', index=False,  encoding=\"utf-8-sig\")\n",
        "    raw_data['label'].to_csv(f'{path}/labels.csv', index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    # Remove smileys and sequential punctuation for medium pre-processing\n",
        "    if pre_process != 'low':\n",
        "      raw_data['medium'] = raw_data['low'].apply(remove_emojis)\n",
        "      raw_data['medium'] = raw_data['medium'].apply(remove_repeats)\n",
        "      raw_data['medium'].to_csv(f'{path}/documents_stripped_medium.csv', index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    # Make reference to the right documents\n",
        "    docs = pre_process\n",
        "\n",
        "    # Initialize pre-processing\n",
        "    if pre_process == 'heavy':\n",
        "      preprocessor = Preprocessing(lowercase=True,\n",
        "          remove_punctuation=True,\n",
        "          punctuation=string.punctuation,\n",
        "          remove_numbers=False,\n",
        "          lemmatize=False,\n",
        "          language=\"english\",\n",
        "          split=False,\n",
        "          verbose=False,\n",
        "          save_original_indexes=True,\n",
        "          remove_stopwords_spacy=False,)\n",
        "      docs = 'medium' # Refer to the right documents\n",
        "    elif pre_process == 'full':\n",
        "      preprocessor = Preprocessing(lowercase=True,\n",
        "                remove_punctuation=True,\n",
        "                punctuation=string.punctuation,\n",
        "                remove_numbers=True,\n",
        "                lemmatize=False,\n",
        "                language=\"english\",\n",
        "                split=False,\n",
        "                verbose=False,\n",
        "                save_original_indexes=True,\n",
        "                remove_stopwords_spacy=False,)\n",
        "      docs = 'medium' # Refer to the right documents\n",
        "    elif pre_process == 'extra':\n",
        "      preprocessor = Preprocessing(lowercase=True,\n",
        "          remove_punctuation=True,\n",
        "          punctuation=string.punctuation,\n",
        "          remove_numbers=True,\n",
        "          lemmatize=True,\n",
        "          language=\"english\",\n",
        "          split=False,\n",
        "          verbose=False,\n",
        "          save_original_indexes=True,\n",
        "          remove_stopwords_spacy=False,)\n",
        "      docs = 'medium' # Refer to the right documents\n",
        "    else:\n",
        "      preprocessor = Preprocessing(lowercase=True,\n",
        "                      remove_punctuation=False,\n",
        "                      punctuation=string.punctuation,\n",
        "                      remove_numbers=False,\n",
        "                      lemmatize=False,\n",
        "                      language=\"english\",\n",
        "                      split=False,\n",
        "                      verbose=False,\n",
        "                      save_original_indexes=True,\n",
        "                      remove_stopwords_spacy=False,)\n",
        "\n",
        "    # Pre-process\n",
        "    dataset = preprocessor.preprocess_dataset(documents_path=f'{path}/documents_stripped_{docs}.csv', labels_path=f'{path}/labels.csv')\n",
        "\n",
        "    # Save the pre-processed dataset\n",
        "    dataset.save(f'{path}/ukraine_{pre_process}')\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FvKwWmUhBPfl"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def embed_messages(embedding_model, dataset, model):\n",
        "\n",
        "  if  os.path.exists(f'/content/drive/MyDrive/2024/{model[:2]}/embeddings.npy'):\n",
        "      with open(f'/content/drive/MyDrive/2024/{model[:2]}/embeddings.npy', 'rb') as f:\n",
        "        embedding = np.load(f)\n",
        "  else:\n",
        "    # Prepare data\n",
        "    data = [\" \".join(words) for words in dataset.get_corpus()]\n",
        "\n",
        "    # Calculate embeddings\n",
        "    embeddings = embedding_model.encode(data, show_progress_bar=True)\n",
        "\n",
        "    # Save embeddings\n",
        "    with open(f'/content/drive/MyDrive/2024/{model[:2]}/embeddings.npy', 'wb') as f:\n",
        "        np.save(f, embeddings)\n",
        "\n",
        "  return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CbrLJ97DAJ7l"
      },
      "outputs": [],
      "source": [
        "def train_model(raw_data, pre_process, embed, n_neighbors, n_components, min_cluster_size, model, word2vec_model, min_samples =10):\n",
        "\n",
        "  # Pre-process data\n",
        "  dataset = pre_process_data(raw_data, pre_process)\n",
        "\n",
        "  # Embed data\n",
        "    # Pick embedder\n",
        "  if embed == 'default':\n",
        "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    embedding_model.max_seq_length = 512\n",
        "  elif embed == 'advanced':\n",
        "    embedding_model = SentenceTransformer('jamesgpt1/sf_model_e5')\n",
        "  embeddings = embed_messages(embedding_model, dataset, model)\n",
        "\n",
        "  memory = Memory(f'/content/drive/MyDrive/2024/HDBSCAN/{model[:7]}', verbose=0) # Save HDBSCAN structure\n",
        "\n",
        "  params = {\n",
        "    'embedding_model': embedding_model,\n",
        "    'verbose': True,\n",
        "    'umap_model': UMAP(n_neighbors=n_neighbors, n_components=n_components, min_dist=0.0, low_memory = False, metric='cosine',random_state=1), # Fix UMAP model with random state\n",
        "    'hdbscan_model': HDBSCAN(min_cluster_size=min_cluster_size, min_samples = min_samples, prediction_data=True, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=True, memory=memory), # min cluster size by default 10\n",
        "    'vectorizer_model': CountVectorizer(stop_words=\"english\", ngram_range=(1, 2)), # default only ngram (1,1)\n",
        "    # 'representation_model': KeyBERTInspired(), # strongly influences measures depending on topic words\n",
        "  }\n",
        "\n",
        "  trainer = Trainer(dataset=f'/content/drive/MyDrive/2024/ukraine_{pre_process}',\n",
        "                  model_name=\"BERTopic\",\n",
        "                  params=params,\n",
        "                  bt_embeddings=embeddings,\n",
        "                  custom_dataset=True,\n",
        "                  verbose=False,\n",
        "                  word2vec_model=word2vec_model,\n",
        "                  semi_supervised=False,\n",
        "                  labels = raw_data['label_num'])\n",
        "  results, topic_model = trainer.train(save=f\"/content/drive/MyDrive/2024/{model}/train_results\")\n",
        "\n",
        "\n",
        "  # Save model\n",
        "  topic_model.save(f'/content/drive/MyDrive/2024/{model}/topic_model.joblib', serialization=\"pickle\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqlGUEEO3v0S"
      },
      "source": [
        "# Functions for evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QugUMTp8WaQX"
      },
      "source": [
        "### Obtain RRI results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bAW5MGJfWgLQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.cluster import pair_confusion_matrix\n",
        "\n",
        "# Calculate RRI\n",
        "\n",
        "def get_RRI(topics, labels, leave_out = [-1]):\n",
        "  \"\"\"\n",
        "  Caluclates the refinement Rand Index score while leaving out some topics\n",
        "\n",
        "  Input:\n",
        "  topics: the refined topics\n",
        "  labels: the original labeling\n",
        "  leave_out: a  list of topic numbers to hold out. By default only exclude outliers\n",
        "\n",
        "  Returns the Refinement Rand Index\n",
        "  \"\"\"\n",
        "  # Exclude the specified topics\n",
        "  labels = labels[~topics.isin(leave_out)]\n",
        "  topics = topics[~topics.isin(leave_out)]\n",
        "\n",
        "  # Measure on refinement; inspired on Rand Index\n",
        "  contingency = pair_confusion_matrix(labels, topics)\n",
        "  RRI = contingency[1][1]/ (contingency[1][1]+contingency[0][1])\n",
        "  return RRI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ju7KqZcZzGw"
      },
      "source": [
        "### Obtain coherence results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IwKGrErC54Of"
      },
      "outputs": [],
      "source": [
        "def get_coherence(data, document_info, topic_model, embeddings, dataset, word2vec_model):\n",
        "  doc_ids = range(len(data))\n",
        "  documents = pd.DataFrame({\"Document\": data,\n",
        "                            \"ID\": doc_ids,\n",
        "                            'Class':  document_info['Topic']})\n",
        "  bertopic_evaluator =  Evaluator(documents, embeddings, topic_model = topic_model, name = 'BERTopic', dataset =dataset, word2vec_model=word2vec_model, verbose = False)\n",
        "  scores = bertopic_evaluator.score(save = None)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HttuPi2Ve3y"
      },
      "source": [
        "### Obtain SRA results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Yq9ap4V9sqCX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "def balanced_subsample(n, raw_data):\n",
        "    n_categories = raw_data['label'].nunique()\n",
        "    counts = raw_data.label.value_counts().sort_values()\n",
        "    sorted_labels = counts.index.tolist()\n",
        "\n",
        "    n_remaining = n\n",
        "    subsample = []\n",
        "\n",
        "    for label, count in counts.items():\n",
        "        samples_per_category = round(n_remaining/n_categories) # Calculate the new number of samples per category\n",
        "        if count < samples_per_category:\n",
        "            sample_size =  count\n",
        "        else:\n",
        "            sample_size = samples_per_category\n",
        "\n",
        "        label_ids = np.where(raw_data['label'] ==  label)[0]\n",
        "        sample_ids = random.sample(label_ids.tolist(), sample_size)\n",
        "        subsample.extend(sample_ids)\n",
        "        n_remaining -= sample_size\n",
        "        n_categories -= 1\n",
        "\n",
        "    return raw_data.low.iloc[subsample]\n",
        "\n",
        "def create_reformulations(raw_data, n, path):\n",
        "  # Returns and saves reformulated messages\n",
        "  prompt_format = \"\"\"\n",
        "  I am doing an experiment to perform semantic similarity, as such, I need a paraphrased message.\n",
        "  I have a message that looks as follows: [MESSAGE]\n",
        "\n",
        "  Consider the above message, reformulate it, but write it from the same perspective.\n",
        "  Deliver the reformulation in the following format:\n",
        "  Message: <reformulation>\"\"\"\n",
        "\n",
        "  subsample = balanced_subsample(n, raw_data)\n",
        "  reformulations = pd.DataFrame({'Original': subsample.to_numpy(), 'Reformulation': np.repeat('', n)})\n",
        "  reformulations.set_index(np.array(subsample.index), inplace=True)\n",
        "\n",
        "  delay_in_seconds=1 # Delay in between subsequent prompts to avoid RateLimitError of OpenAI\n",
        "\n",
        "\n",
        "  for i, message in enumerate(reformulations.Original):\n",
        "    prompt = prompt_format.replace('[MESSAGE]', message)\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}]\n",
        "    kwargs = {\"model\": deployment_name, \"messages\": messages}\n",
        "\n",
        "    try:\n",
        "      response = client.chat.completions.create(**kwargs)\n",
        "      reformulation = response.choices[0].message.content.strip().replace(\"Message: \", \"\")\n",
        "\n",
        "      # Remove remaining introduction\n",
        "      reformulation = reformulation.replace(\"<Reformulation>\",\"\").strip()\n",
        "      reformulation = reformulation.replace(\"<reformulation>\",\"\")\n",
        "      reformulation = reformulation.replace(\"<Message>\",\"\")\n",
        "      reformulation = reformulation.replace(\"<message>\",\"\")\n",
        "      reformulation = reformulation.replace(\"Reformulation:\",\"\")\n",
        "      reformulation = reformulation.replace(\"reformulation:\",\"\")\n",
        "      reformulation = reformulation.replace(\"Message:\",\"\")\n",
        "      reformulation = reformulation.replace(\"message:\",\"\")\n",
        "      reformulation = reformulation.replace(\"Reformulated \",\"\")\n",
        "\n",
        "      reformulations.Reformulation.iloc[i] = reformulation\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      reformulations.Reformulation.iloc[i] = 'ERROR_FLAG'\n",
        "\n",
        "    time.sleep(delay_in_seconds)\n",
        "\n",
        "  # Store because of expensive recalculation\n",
        "  if not os.path.exists(f'{path}'):\n",
        "    os.makedirs(f'{path}')\n",
        "  reformulations.to_csv(f'{path}.csv', index=True, encoding=\"utf-8-sig\")\n",
        "\n",
        "  return reformulations\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_SRA(topic_model, document_info, model, pre_process, embed, reformulations = None, raw_data = None, n=20, save_to ='/content/drive/MyDrive/2024/reformulations'):\n",
        "  # Either input reformulations or raw_data and n\n",
        "  # Calculates the SRA score\n",
        "\n",
        "  # Obtain reformulated messages\n",
        "  if reformulations is None:\n",
        "    reformulations = create_reformulations(raw_data, n, path = save_to)\n",
        "\n",
        "  # Sometimes creates empty reformulation\n",
        "  reformulations= reformulations[reformulations.Reformulation !=''] # If generated\n",
        "  reformulations = reformulations.dropna() # If loaded\n",
        "  reformulations= reformulations[reformulations.Reformulation !='ERROR_FLAG'] # Remove error flags\n",
        "\n",
        "  # Pre-process reformulations\n",
        "  test_data = reformulations.rename(columns ={'Reformulation':'low'})\n",
        "  test_data['label'] = np.repeat('', len(reformulations)) # Create fake label data in order to be able to apply pre-processing\n",
        "  test_data = pre_process_data(test_data, pre_process, reformulations_path = save_to)\n",
        "  test_data = test_data.get_corpus()\n",
        "  test_data = [\" \".join(words) for words in test_data]\n",
        "\n",
        "  # Calculate embeddings\n",
        "  # Pick embedder\n",
        "  if embed == 'default':\n",
        "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    embedding_model.max_seq_length = 512\n",
        "  elif embed == 'advanced':\n",
        "    embedding_model = SentenceTransformer('jamesgpt1/sf_model_e5')\n",
        "\n",
        "  if not os.path.exists(f'{save_to}/{model}_embeddings.npy'):\n",
        "    test_embedding = embedding_model.encode(test_data, show_progress_bar=False)\n",
        "    with open(f'{save_to}/{model}_embeddings.npy', 'wb') as f:\n",
        "      np.save(f, test_embedding)\n",
        "  with open(f'{save_to}/{model}_embeddings.npy', 'rb') as f:\n",
        "      test_embedding = np.load(f)\n",
        "\n",
        "  # Obtain results for reformulations\n",
        "  reformulation_topics, reformulation_dist = topic_model.transform(test_data, test_embedding) # OUPTPUT DIFFERS WITH WHICH OTHER MESSAGES A REFORMULATION IS PROCESSED\n",
        "\n",
        "  # Calculate SRA\n",
        "  cos_sim = np.diag(cosine_similarity(reformulation_dist, topic_model.probabilities_[reformulations.index]))\n",
        "  SRA = np.mean(cos_sim)\n",
        "\n",
        "  # Calculate average certainty\n",
        "  certainty = reformulation_dist[np.arange(len(reformulation_topics)),reformulation_topics]\n",
        "  certainty = certainty[np.array(reformulation_topics) != -1] # Exclude outliers from analysis\n",
        "  certainty = np.mean(certainty)\n",
        "\n",
        "  # Calculate f1\n",
        "  true_topics = document_info.Topic.iloc[reformulations.index]\n",
        "  f1 = f1_score(true_topics, reformulation_topics, average = 'macro')\n",
        "\n",
        "  # Backhand calculations\n",
        "  SRA_info = reformulations.copy()\n",
        "  SRA_info['Original topic'] = true_topics\n",
        "  SRA_info['Reformulation topic']=  reformulation_topics\n",
        "  SRA_info['cos_sim'] = cos_sim\n",
        "\n",
        "  return SRA, SRA_info, certainty, f1\n",
        "\n",
        "def SRA_results(raw_data, document_info, topic_model, model, pre_process, embed, repeats =1, existig_reformulations = True, n=20): # Boolean if saved reformulations can be used\n",
        "  SRAs = []\n",
        "  certainties = []\n",
        "  f1s = []\n",
        "\n",
        "  for run in range(repeats):\n",
        "    path  = f'/content/drive/MyDrive/2024/reformulations/{run}'\n",
        "    if existig_reformulations:\n",
        "      if run in [48,49]:\n",
        "        reformulations = pd.read_csv(f'{path}.csv', index_col =0, sep=';', usecols=range(0,3)) # Hard code different format reformulations\n",
        "      else:\n",
        "        reformulations = pd.read_csv(f'{path}.csv', index_col =0)\n",
        "      SRA, SRA_info, certainty, f1 = calculate_SRA(topic_model, document_info, model, pre_process, embed, reformulations = reformulations, save_to = path)\n",
        "    else:\n",
        "      SRA, SRA_info, certainty, f1 = calculate_SRA(topic_model, document_info, model, pre_process, embed, raw_data = raw_data, n=n, save_to = path)\n",
        "    SRAs.append(SRA)\n",
        "    certainties.append(certainty)\n",
        "    f1s.append(f1)\n",
        "\n",
        "  # Show insights of last run\n",
        "  return SRAs, SRA_info, np.mean(certainties), np.mean(f1s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Emg1sCdFpgB"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IUgAKm3JxDnf"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "raw_data = pd.read_csv('/content/drive/MyDrive/2024/raw_data.csv', encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6V_-FbHXJI4N"
      },
      "outputs": [],
      "source": [
        "# Load word2vec model\n",
        "from gensim.models import KeyedVectors\n",
        "word2vec_model = KeyedVectors.load('/content/drive/MyDrive/2024/word2vec_model.kv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SvaogtAI4hE5"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(pre_process, embed, n_neighbors, n_components, min_cluster_size, raw_data, word2vec_model, existig_reformulations = True):\n",
        "  # Pick model\n",
        "  if pre_process == 'low' and embed == 'default':\n",
        "    model = 'LD'\n",
        "  elif pre_process == 'medium' and embed == 'default':\n",
        "    model = 'MD'\n",
        "  elif pre_process == 'heavy' and embed == 'default':\n",
        "    model = 'HD'\n",
        "  elif pre_process == 'full' and embed == 'default':\n",
        "    model = 'FD'\n",
        "  elif pre_process == 'low' and embed == 'advanced':\n",
        "    model = 'LA'\n",
        "  elif pre_process == 'medium' and embed == 'advanced':\n",
        "    model = 'MA'\n",
        "  elif pre_process == 'heavy' and embed == 'advanced':\n",
        "    model = 'HA'\n",
        "  elif pre_process == 'full' and embed == 'advanced':\n",
        "    model = 'FA'\n",
        "\n",
        "  model = model + '_' +str(n_neighbors) + '_' + str(n_components) + '_' + str(min_cluster_size)\n",
        "\n",
        "  # Check if model has been trained already\n",
        "  new_model = not os.path.exists(f\"/content/drive/MyDrive/2024/{model}\")\n",
        "  if new_model:\n",
        "    os.makedirs(f\"/content/drive/MyDrive/2024/{model}\")\n",
        "    train_model(raw_data, pre_process = pre_process, embed = embed, n_neighbors=n_neighbors, n_components=n_components, min_cluster_size=min_cluster_size, model = model, word2vec_model=word2vec_model)\n",
        "    print('Finished training '+model)\n",
        "  else:\n",
        "    print('Loading '+model)\n",
        "\n",
        "  # Load pre-processed dataset\n",
        "  dataset = Dataset('ukraine')\n",
        "  dataset.load_custom_dataset_from_folder(f'/content/drive/MyDrive/2024/ukraine_{pre_process}')\n",
        "  list_data = dataset.get_corpus()\n",
        "  data = [\" \".join(words) for words in list_data]\n",
        "\n",
        "  # Load embeddings\n",
        "  with open(f'/content/drive/MyDrive/2024/{model[:2]}/embeddings.npy', 'rb') as f:\n",
        "    embeddings = np.load(f)\n",
        "\n",
        "  # Load model\n",
        "  topic_model = BERTopic.load(f'/content/drive/MyDrive/2024/{model}/topic_model.joblib')\n",
        "\n",
        "  # Obtain results\n",
        "  document_info = topic_model.get_document_info(data)\n",
        "\n",
        "  outliers = sum(document_info.Topic == -1)\n",
        "\n",
        "  # Get metrics\n",
        "  RRI = get_RRI(document_info['Topic'], raw_data['label'])\n",
        "  coherence_scores = get_coherence(data, document_info, topic_model, embeddings,dataset, word2vec_model)\n",
        "  DBCV = topic_model.hdbscan_model.relative_validity_\n",
        "\n",
        "  # Save the values of all metrics\n",
        "  scores = coherence_scores\n",
        "  scores['RRI'] = RRI\n",
        "  scores[ 'DBCV'] = DBCV\n",
        "  SRAs, SRA_info, certainty, f1 = SRA_results(raw_data, document_info, topic_model, model[:2], pre_process, embed, repeats =50, existig_reformulations = existig_reformulations, n=100)\n",
        "  scores['SRA'] = np.mean(SRAs)\n",
        "  scores['CI SRA'] = sms.DescrStatsW(SRAs).tconfint_mean()\n",
        "  scores['Certainty'] = certainty\n",
        "  scores['F1'] = f1\n",
        "  with open(f\"/content/drive/MyDrive/2024/{model}/results.json\", \"w\") as f:\n",
        "      json.dump(scores, f)\n",
        "  print(scores)\n",
        "  scores['Model'] = model\n",
        "  scores['Outliers'] = outliers\n",
        "\n",
        "  with open('/content/drive/MyDrive/2024/overview.csv', 'a') as f_object:\n",
        "      dictwriter_object = DictWriter(f_object, fieldnames=list(scores.keys()))\n",
        "      dictwriter_object.writerow(scores)\n",
        "      f_object.close()\n",
        "\n",
        "  return topic_model, document_info,  SRAs, SRA_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "955287429c464022a04b46d5848ecb24",
            "04385500d4e14fa9a8ee156bd505a597",
            "ed9163ea269a40f0adbf2bdd17c2a7e8",
            "23465036a2134481993ae33e4d6c5151",
            "8216505151f64da9b11f2f47c2ded63e",
            "98ab3054666449db8a9eafd2c5223ccf",
            "fe6bc4bf1b654aea9f9abc41eb645190",
            "49b98aa29c574088ba22604ad4da9759",
            "a406b18504054d6da3929bfeca4725f2",
            "a62024898915400f83193afb9aac0d9a",
            "4a4bfe7848364deb9c9c28a7b09bb80d"
          ]
        },
        "id": "LqMMwPLfy7Mv",
        "outputId": "62545e87-af27-49c2-a317-aa4a1e297225"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/142 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "955287429c464022a04b46d5848ecb24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-02-27 15:00:08,850 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2024-02-27 15:00:20,833 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:00:20,836 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2024-02-27 15:00:22,451 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:00:22,472 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "2024-02-27 15:00:23,831 - BERTopic - Representation - Completed ✓\n",
            "2024-02-27 15:00:24,619 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished training MA_15_5_50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-02-27 15:01:17,886 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:19,022 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:19,025 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:19,041 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:19,083 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:19,085 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:20,764 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:21,950 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:21,951 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:21,964 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:22,004 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:22,007 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:23,540 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:25,223 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:25,226 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:25,248 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:25,301 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:25,310 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:27,728 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:28,787 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:28,789 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:28,803 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:28,835 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:28,836 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:30,715 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:32,267 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:32,269 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:32,279 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:32,300 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:32,301 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:33,659 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:34,449 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:34,450 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:34,460 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:34,489 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:34,491 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:36,084 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:36,888 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:36,890 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:36,898 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:36,922 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:36,923 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:38,514 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:39,698 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:39,701 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:39,709 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:39,746 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:39,748 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:41,174 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:41,957 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:41,958 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:41,967 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:41,988 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:41,989 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:43,327 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:44,101 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:44,102 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:44,111 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:44,133 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:44,134 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:45,610 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:46,398 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:46,399 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:46,409 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:46,434 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:46,436 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:47,872 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:49,419 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:49,420 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:49,430 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:49,451 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:49,452 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:51,147 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:52,403 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:52,408 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:52,419 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:52,463 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:52,466 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:53,972 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:54,776 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:54,778 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:54,786 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:54,806 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:54,807 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:56,292 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:57,089 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:57,090 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:57,100 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:57,124 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:57,125 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:01:58,705 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:01:59,493 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:01:59,495 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:01:59,505 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:01:59,529 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:01:59,530 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:00,901 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:01,686 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:01,687 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:01,699 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:01,720 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:01,721 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:03,107 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:04,318 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:04,323 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:04,333 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:04,366 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:04,369 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:05,990 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:07,536 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:07,538 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:07,551 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:07,572 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:07,573 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:09,028 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:09,801 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:09,802 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:09,812 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:09,839 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:09,840 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:11,369 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:12,173 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:12,174 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:12,184 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:12,205 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:12,206 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:13,735 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:14,518 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:14,519 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:14,528 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:14,551 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:14,552 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:16,733 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:17,969 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:17,973 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:17,986 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:18,024 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:18,028 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:19,620 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:20,439 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:20,440 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:20,450 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:20,472 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:20,473 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:22,083 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:22,884 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:22,885 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:22,895 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:22,919 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:22,919 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:24,306 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:25,094 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:25,096 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:25,105 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:25,127 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:25,128 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:26,495 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:28,047 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:28,048 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:28,058 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:28,079 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:28,080 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:29,518 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:30,716 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:30,718 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:30,728 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:30,760 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:30,761 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:32,311 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:33,133 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:33,134 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:33,144 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:33,166 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:33,167 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:34,560 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:35,343 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:35,345 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:35,354 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:35,375 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:35,376 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:36,780 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:37,580 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:37,581 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:37,592 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:37,617 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:37,618 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:40,214 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:40,996 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:40,998 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:41,008 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:41,034 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:41,035 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:42,587 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:43,813 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:43,820 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:43,830 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:43,873 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:43,875 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:45,410 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:46,948 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:46,949 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:46,959 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:46,981 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:46,982 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:48,327 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:49,123 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:49,125 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:49,134 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:49,156 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:49,157 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:50,719 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:51,505 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:51,506 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:51,515 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:51,537 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:51,538 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:52,982 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:53,774 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:53,776 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:53,786 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:53,807 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:53,808 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:55,161 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:56,386 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:56,388 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:56,398 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:56,431 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:56,435 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:02:58,105 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:02:58,885 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:02:58,887 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:02:58,895 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:02:58,919 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:02:58,920 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:03:00,251 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:03:01,042 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:03:01,043 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:03:01,053 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:03:01,074 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:03:01,075 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:03:02,521 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:03:03,309 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:03:03,311 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:03:03,320 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:03:03,341 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:03:03,342 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:03:05,547 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:03:06,330 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:03:06,332 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:03:06,341 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:03:06,362 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:03:06,363 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:03:07,738 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:03:08,824 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:03:08,826 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:03:08,834 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:03:08,875 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:03:08,877 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:03:10,508 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:03:11,453 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:03:11,455 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:03:11,464 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:03:11,494 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:03:11,496 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:03:12,902 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:03:14,314 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:03:14,327 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:03:14,338 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:03:14,407 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:03:14,419 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:03:15,886 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:03:16,683 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:03:16,684 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:03:16,693 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:03:16,716 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:03:16,717 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:03:18,031 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:03:18,806 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:03:18,808 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:03:18,817 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:03:18,837 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:03:18,838 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:03:20,260 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:03:21,261 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:03:21,263 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:03:21,274 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:03:21,315 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:03:21,317 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:03:23,024 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:03:24,781 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:03:24,782 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:03:24,791 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:03:24,813 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:03:24,813 - BERTopic - Cluster - Completed ✓\n",
            "2024-02-27 15:03:26,131 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
            "2024-02-27 15:03:26,943 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-02-27 15:03:26,944 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
            "2024-02-27 15:03:26,953 - BERTopic - Probabilities - Start calculation of probabilities with HDBSCAN\n",
            "2024-02-27 15:03:26,975 - BERTopic - Probabilities - Completed ✓\n",
            "2024-02-27 15:03:26,976 - BERTopic - Cluster - Completed ✓\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'we': 0.034758882458639326, 'diversity': 0.5862068965517241, 'RRI': 0.42025870849460173, 'DBCV': 0.30422572631864864, 'SRA': 0.8583542042894152, 'CI SRA': (0.8516444338855905, 0.8650639746932399), 'Certainty': 0.8042137177433442, 'F1': 0.8809541455666143}\n"
          ]
        }
      ],
      "source": [
        "new_reformulations = False\n",
        "\n",
        "for pre_process in ['medium']:\n",
        "  for embed in ['advanced']:\n",
        "    i = 0\n",
        "    for n_neighbors in [15,30]:\n",
        "      for n_components in [5,20]:\n",
        "        for min_cluster_size in [10,50]:\n",
        "          if i > 0: # first is default so not needed to run again\n",
        "            topic_model, document_info, SRAs, SRA_info= run_pipeline(pre_process, embed, n_neighbors, n_components, min_cluster_size, raw_data, word2vec_model, existig_reformulations = not new_reformulations)\n",
        "            new_reformulations = False # At most once needed to create the reformulations for testing multiple models\n",
        "          i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJAT2kx3EhZ"
      },
      "source": [
        "# Load data in right format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJV_dOAJESFs"
      },
      "outputs": [],
      "source": [
        "# # Load data\n",
        "# raw_data = pd.read_excel('/content/drive/MyDrive/messages_2023_list.xlsx')\n",
        "\n",
        "# # Remove enters in label names\n",
        "# raw_data['label'] =  raw_data['label'].apply(lambda x: re.sub(r'\\n','', str(x)))\n",
        "\n",
        "# # Create class numbers for each label for semi-supervised learning\n",
        "# label_to_num = {k: v for v, k in enumerate(pd.Series(raw_data['label']).unique())}\n",
        "# raw_data['label_num'] = raw_data['label'].map(label_to_num)\n",
        "\n",
        "# # Remove urls\n",
        "# raw_data['text'] = raw_data['text'].apply(lambda x: re.sub(r'\\(https[^\\)]*\\)',' <URL> ', str(x))) # Remove based on brackets\n",
        "# raw_data['text'] = raw_data['text'].apply(lambda x: re.sub(r'https[\\S]*','<URL>', str(x))) # Remove based on followed by a space or enter\n",
        "\n",
        "# # Remove any remaining Ukrainian text\n",
        "# raw_data['low'] =  raw_data['text'].apply(lambda x: re.sub(r'\\p{IsCyrillic}','', str(x)))\n",
        "\n",
        "# # Save required data\n",
        "# raw_data.to_csv('/content/drive/MyDrive/2024/raw_data.csv', index=False, encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN70_dvTE8VZ"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import display, HTML\n",
        "\n",
        "# def pretty_print(df):\n",
        "#   # Prints DataFrames in a neat format\n",
        "#   return display( HTML( df.to_html().replace(\"\\\\n\",\"<br>\")))\n",
        "\n",
        "# # Print examples from pre-processing\n",
        "\n",
        "# raw_data_examples = pd.read_excel('/content/drive/MyDrive/messages_2023_list_old.xlsx')\n",
        "\n",
        "# examples = raw_data_examples[['text', 'label']].iloc[[565,1448,456]]\n",
        "\n",
        "# pretty_print(examples)\n",
        "\n",
        "# # Remove any remaining Ukrainian text\n",
        "# raw_data_examples['low'] =  raw_data_examples['text'].apply(lambda x: re.sub(r'\\p{IsCyrillic}','', str(x)))\n",
        "\n",
        "# pretty_print(pd.DataFrame(raw_data_examples[['text', 'low']].iloc[2266]).transpose().rename(columns={'text':'Original', 'low': 'Pre-processed'}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SfSG8rCe3zY"
      },
      "outputs": [],
      "source": [
        "# import gensim.downloader as api\n",
        "\n",
        "# word2vec_model = api.load('word2vec-google-news-300')\n",
        "# word2vec_model.save('/content/drive/MyDrive/2024/word2vec_model.kv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr_F3GLX3bAg"
      },
      "source": [
        "# Inspect data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5dtn9N3Jq4W"
      },
      "outputs": [],
      "source": [
        "# Analyze data\n",
        "\n",
        "# # Overview of categories\n",
        "# plt.figure().set_figheight(7)\n",
        "# sns.countplot(y = 'label', data=raw_data, order = raw_data['label'].value_counts().index)\n",
        "# plt.title('Messages per category')\n",
        "# plt.xlabel('Amount of messages')\n",
        "# plt.ylabel('Label')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfbQe5SCyGee"
      },
      "outputs": [],
      "source": [
        "# #%%\n",
        "# words = [re.sub('[^a-z]','',word.lower()) for message in list_data for word in message] # Remove punctuation and digits and lowercase all words\n",
        "# no_stops = [word for word in words if word not in stopwords.words('english')] # Exclude stopwords\n",
        "# counts = Counter(no_stops)\n",
        "# print(counts.most_common(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBD6LtALSfeI"
      },
      "outputs": [],
      "source": [
        "# # Analyse lenght of messages\n",
        "# lengths = [len(message) for message in list_data]\n",
        "# print('Max lenght:', max(lengths))\n",
        "# print('Min lenght:', min(lengths))\n",
        "# longest_messages = (-np.array(lengths)).argsort()[:10]\n",
        "# sns.histplot(lengths)\n",
        "# plt.title('Distribution of message sizes')\n",
        "# plt.xlabel('Number of words')\n",
        "# plt.ylabel('Amount of messages')\n",
        "# plt.xticks(np.arange(0,550,50))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "L1rjck2Hvy_e",
        "outputId": "d37d639a-8665-46b3-d9c3-c5cd52e375f5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'embedding_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-a53631544523>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Tokenize sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding_model' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "tokenizer = embedding_model.tokenizer\n",
        "\n",
        "# Tokenize sentences\n",
        "tokenized_messages = tokenizer(data, padding=True, truncation=False, return_tensors='pt')\n",
        "\n",
        "# Check how many messages cut-off by truncation of SBERT\n",
        "\n",
        "num_tokens = torch.sum(tokenized_messages['input_ids'] > 0, dim=1)\n",
        "# sns.histplot(num_tokens)\n",
        "num_tokens = pd.Series(num_tokens)\n",
        "bins = [0,256,512,1000] # Default truncation is after 256 and can be adjusted to max 512\n",
        "print(num_tokens.value_counts(bins=bins))\n",
        "\n",
        "# Truncation has barely any result. Also because urls blanked and digits removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwDZ4d0wWfbS"
      },
      "outputs": [],
      "source": [
        "# documents = pd.DataFrame({\"Document\": data,\n",
        "#                           \"ID\": doc_ids,\n",
        "#                           'Class':  raw_data['label']})\n",
        "\n",
        "# baseline_evaluator =  Evaluator(documents, embeddings, topic_model = BERTopic(umap_model= umap_model), name = 'Original labeling', dataset =dataset, word2vec_model=word2vec_model)\n",
        "\n",
        "# baseline_evaluator.score(save= f\"/content/drive/MyDrive/2024/baseline_results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giYKQlMigT33"
      },
      "outputs": [],
      "source": [
        "# # Remove multiple occurences of the same character\n",
        "# # from Tweettokenizer\n",
        "#     pattern = re.compile(r\"(.)\\1{2,}\")\n",
        "#     return pattern.sub(r\"\\1\\1\\1\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPXqK1LxNHR2"
      },
      "outputs": [],
      "source": [
        "def check_cluster(num):\n",
        "  # Check specific cluster\n",
        "  return (document_info['Document'][document_info['Topic']== num])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF1JZN2ELPce"
      },
      "outputs": [],
      "source": [
        "# # Briefly inspect Introdcution effect\n",
        "\n",
        "# example_reformulations = pd.read_csv('/content/drive/MyDrive/2024/reformulations.csv', index_col =0)\n",
        "# SRA, SRA_info = calculate_SRA(topic_model,reformulations = example_reformulations)\n",
        "# print(SRA_info.cos_sim.iloc[-1])\n",
        "\n",
        "# cleaned =  example_reformulations.copy()\n",
        "# alternative = cleaned.Reformulation.iloc[-1].replace('<Message>\\n', '')\n",
        "# cleaned.Reformulation.iloc[-1] = alternative\n",
        "# SRA, SRA_info = calculate_SRA(topic_model,reformulations = cleaned)\n",
        "# SRA_info.cos_sim.iloc[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYlwTBdcJqDL"
      },
      "outputs": [],
      "source": [
        "print('Found topic for reformulation:')\n",
        "print(topic_model.get_topic_info()[['Topic', 'Representation']].iloc[[5+1]].to_string())\n",
        "print(check_cluster(5))\n",
        "\n",
        "print('\\n Reformulation in outlier: ')\n",
        "print(topic_model.get_topic_info()[['Topic', 'Representation']].iloc[[4+1]].to_string())\n",
        "print(check_cluster(4))\n",
        "\n",
        "print('\\n Confuse topics:')\n",
        "print(topic_model.get_topic_info()[['Topic', 'Representation']].iloc[[2+1,56+1]].to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeFDL2dZmcFw"
      },
      "outputs": [],
      "source": [
        "#%%\n",
        "\n",
        "# Analyse results\n",
        "print(topic_model.get_topic_info())\n",
        "print(document_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THoWsFjKyylJ"
      },
      "outputs": [],
      "source": [
        "# from OpenAI_BERTopic_repo import OpenAI\n",
        "\n",
        "# summarization_prompt = \"\"\"\n",
        "# This is a list of texts where each collection of texts describe a topic. After each collection of texts, a summary that captures the most important content of the texts in the collections is provided.\n",
        "# ---\n",
        "# Topic:\n",
        "# Sample texts from this topic:\n",
        "# - Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
        "# - Meat, but especially beef, is the worst food in terms of emissions.\n",
        "# - Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
        "\n",
        "# Keywords: meat beef eat eating emissions steak food health processed chicken\n",
        "# Summary: Meat has become a bigger component of our diets. Eating meat is harmful for the enviroment.\n",
        "# ---\n",
        "# Topic:\n",
        "# Sample texts from this topic:\n",
        "# - I have ordered the product weeks ago but it still has not arrived!\n",
        "# - The website mentions that it only takes a couple of days to deliver but I still have not received mine.\n",
        "# - I got a message stating that I received the monitor but that is not true!\n",
        "# - It took a month longer to deliver than was advised...\n",
        "\n",
        "# Keywords: deliver weeks product shipping long delivery received arrived arrive week\n",
        "# Summary: Some packages take a long time to get delivered. False claims about the delivery of packages are being made.\n",
        "# ---\n",
        "# Topic:\n",
        "# Sample texts from this topic:\n",
        "# [DOCUMENTS]\n",
        "# Keywords: [KEYWORDS]\n",
        "# Summary:\"\"\"\n",
        "\n",
        "# representation_model= OpenAI(client,model = deployment_name, prompt = summarization_prompt, nr_docs=20, delay_in_seconds=3)\n",
        "# representation_model.extract_topics(topic_model, documents.rename(columns={'Class':'Topic'}), topic_model.c_tf_idf_, topic_model.topic_representations_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCaL17hiCu8G"
      },
      "outputs": [],
      "source": [
        "# Select cluster to analyse\n",
        "cluster_num = 36"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-C2_c7RrdUF"
      },
      "outputs": [],
      "source": [
        "from OpenAI_BERTopic_repo import OpenAI\n",
        "\n",
        "summarization_chat_prompt = \"\"\"\n",
        "I have a topic that is described by the following keywords: [KEYWORDS]\n",
        "In this topic, the following documents are a small but representative subset of all documents in the topic:\n",
        "[DOCUMENTS]\n",
        "\n",
        "Based on the information above, please write a summary that captures the most important content of this topic in the following format:\n",
        "topic: <summary>\n",
        "\"\"\"\n",
        "\n",
        "representation_model = OpenAI(client, model=deployment_name, chat=True, prompt=summarization_chat_prompt, nr_docs=20, delay_in_seconds=3)\n",
        "representation_model.extract_topics(topic_model, documents[documents['Class']==cluster_num].rename(columns={'Class':'Topic'}), topic_model.c_tf_idf_.getrow(cluster_num), {cluster_num:topic_model.topic_representations_[cluster_num]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LobMg5JouO0T"
      },
      "outputs": [],
      "source": [
        "print(topic_model.get_topic_freq().head(11))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWgVr-rPGVeG"
      },
      "outputs": [],
      "source": [
        "# Overview of cluster sizes\n",
        "sns.barplot(y = 'Count', x = 'Topic', data=topic_model.get_topic_freq().iloc[1:])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAox2c5pH0Mg"
      },
      "outputs": [],
      "source": [
        "# Check biggest cluster\n",
        "print(document_info['Document'][document_info['Topic']== 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZoOyXlKTlyc"
      },
      "outputs": [],
      "source": [
        "# Analyze clusters of large text messages\n",
        "\n",
        "print(raw_data['text'].iloc[longest_messages])\n",
        "print(document_info['Topic'].iloc[longest_messages])\n",
        "\n",
        "# Check specific cluster\n",
        "print('Longest cluster:', document_info['Top_n_words'].iloc[longest_messages[0]])\n",
        "print(document_info['Document'][document_info['Topic']== document_info['Topic'].iloc[longest_messages[0]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOf-zjtXuSaE"
      },
      "outputs": [],
      "source": [
        "fig = topic_model.visualize_topics()\n",
        "fig.write_html(\"inter.html\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66OjoHixuVEc"
      },
      "outputs": [],
      "source": [
        "heatmap = topic_model.visualize_heatmap()\n",
        "heatmap.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_5Rw9giucyx"
      },
      "outputs": [],
      "source": [
        "# Check specific cluster\n",
        "print(check_cluster(74))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgI-qIOpusRE"
      },
      "outputs": [],
      "source": [
        "#%%\n",
        "\n",
        "# Check if not focussing on marker tokens\n",
        "# Could potentionally check with tf-idf score\n",
        "# Now because of preprocessing punctuation is removed, so markers not this specific anymore\n",
        "\n",
        "pers_topics = pd.concat([document_info['Topic'],raw_data['text'].str.contains('<PERSON>')],axis=1)\n",
        "freq_pers = pers_topics.groupby(['Topic'], as_index=False).mean()\n",
        "sns.barplot(x = 'Topic', y= 'text', data=freq_pers, color='b')\n",
        "plt.title('<PERSON> marker')\n",
        "plt.ylabel('Percetage contains <PERSON>')\n",
        "plt.show()\n",
        "\n",
        "url_topics = document_info['Topic'][raw_data['text'].str.contains('<URL>')]\n",
        "sns.histplot(url_topics, binwidth=1)\n",
        "plt.title('<URL> marker')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6BIjeaz8ofw"
      },
      "outputs": [],
      "source": [
        "# Get matrix with for every word the importance for that topic and the corresponding words\n",
        "topic_term_matrix = topic_model.c_tf_idf_.toarray()\n",
        "words = topic_model.vectorizer_model.get_feature_names_out()\n",
        "\n",
        "# Get the column corresponding to person\n",
        "person_col = np.where(words=='person')[0]\n",
        "\n",
        "# Get for each topic the ranking on how important person is\n",
        "person_importance = np.where(np.argsort(-topic_term_matrix)==person_col)[1]\n",
        "\n",
        "# Get the ranking for the topic where it's most important (exclude the outliers)\n",
        "max_importance = np.min(person_importance[1:])\n",
        "print(max_importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muUxAl0pFzXd"
      },
      "outputs": [],
      "source": [
        "KK_topics = pd.concat([document_info['Topic'],raw_data['text'].str.contains(' KK ')],axis=1)\n",
        "freq_KK = KK_topics.groupby(['Topic'], as_index=False).mean()\n",
        "sns.barplot(x = 'Topic', y= 'text', data=freq_KK, color='b')\n",
        "plt.title('KK')\n",
        "plt.ylabel('Percetage contains KK')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftn5GcixvNPj"
      },
      "outputs": [],
      "source": [
        "#%%\n",
        "\n",
        "# Ditribution over topics\n",
        "\n",
        "topic_distr, _ = topic_model.approximate_distribution(data, window=8, stride=4)\n",
        "topic_model.visualize_distribution(topic_distr[0], custom_labels=True)\n",
        "\n",
        "# Maybe nice for long messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vPro2MRwYiU"
      },
      "outputs": [],
      "source": [
        "instance = 3121\n",
        "\n",
        "# Calculate the topic distributions on a token-level\n",
        "topic_distr, topic_token_distr = topic_model.approximate_distribution(data, calculate_tokens=True)\n",
        "\n",
        "# Visualize the token-level distributions\n",
        "topic_model.visualize_approximate_distribution(data[instance], topic_token_distr[instance])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbu5mt1yw94J"
      },
      "outputs": [],
      "source": [
        "topics_per_class = topic_model.topics_per_class(raw_data['text'], classes=raw_data['label'])\n",
        "\n",
        "fig = topic_model.visualize_topics_per_class(topics_per_class, top_n_topics=None)\n",
        "# fig.write_html(\"class.html\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv_ce_o9_eTd"
      },
      "outputs": [],
      "source": [
        "raw_data[['text', 'label']][document_info.Topic==79]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJuCNyNr0bdK"
      },
      "outputs": [],
      "source": [
        "hierarchical_topics = topic_model.hierarchical_topics(raw_data['text'])\n",
        "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3dFQ72tB56q"
      },
      "outputs": [],
      "source": [
        "# Assign an original label to topics if more than a threshold percentage of the messages in the topic originate from that label\n",
        "\n",
        "threshold = 0.7\n",
        "raw_data['topic']= document_info['Top_n_words']\n",
        "clustered_only = raw_data[document_info['Topic']!=-1] # Exclude outliers\n",
        "\n",
        "# Count the number of occurences of each label within each topic\n",
        "label_count = clustered_only[['topic', 'label']].groupby(['topic', 'label']).size().reset_index(name='count')\n",
        "\n",
        "# Obtain the frequency how much of the topic originated from which label\n",
        "label_count['freq'] = label_count.groupby(['topic'])['count'].apply(lambda x: x / x.sum()) # DataFrame containing per topic the distribution over the original labels\n",
        "\n",
        "# Say a topic is a refinement if more that the threshold originated from the same label\n",
        "clear_refined = label_count[label_count['freq']>=threshold][['label', 'freq', 'topic']]\n",
        "print(str(len(clear_refined)), ' topics originate clearly from refinement on labeling')\n",
        "print(clear_refined.groupby(['label'])['label', 'topic'].apply(print))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lio4tzzRj6fA"
      },
      "outputs": [],
      "source": [
        "# Get topics that are not from a parcticular label\n",
        "\n",
        "potential_new = label_count[label_count['freq']<threshold][['label', 'freq', 'topic']]\n",
        "covered_topics = clear_refined['topic'].tolist()\n",
        "\n",
        "# Exclude refinement topics\n",
        "potential_new = potential_new[~potential_new['topic'].isin(covered_topics)]\n",
        "\n",
        "# Order topics based on being least certain to be part of the refinement\n",
        "potential_new = potential_new.groupby('topic')['freq'].max().reset_index(name='certainty') # Get the assignment score to the label it was most likely to occur\n",
        "potential_new = potential_new.sort_values('certainty')\n",
        "print(potential_new)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPDAN5kNIQrx"
      },
      "outputs": [],
      "source": [
        "document_info['Document'][document_info['Top_n_words'] ==  potential_new['topic'].iloc[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8hyEPhKR5b7"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import silhouette_score\n",
        "\n",
        "# # Obtain the umap embeddings\n",
        "# umap_embeddings = topic_model.umap_model.transform(embeddings)\n",
        "\n",
        "# # Exclude outliers\n",
        "# umap_embeddings = umap_embeddings[document_info['Topic']!=-1]\n",
        "# clustering = document_info['Topic'][document_info['Topic']!=-1]\n",
        "\n",
        "# # Calculate silhouette score\n",
        "# print(silhouette_score(umap_embeddings, clustering))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzi/BzpEtCajnu+6DPImtb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "955287429c464022a04b46d5848ecb24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04385500d4e14fa9a8ee156bd505a597",
              "IPY_MODEL_ed9163ea269a40f0adbf2bdd17c2a7e8",
              "IPY_MODEL_23465036a2134481993ae33e4d6c5151"
            ],
            "layout": "IPY_MODEL_8216505151f64da9b11f2f47c2ded63e"
          }
        },
        "04385500d4e14fa9a8ee156bd505a597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98ab3054666449db8a9eafd2c5223ccf",
            "placeholder": "​",
            "style": "IPY_MODEL_fe6bc4bf1b654aea9f9abc41eb645190",
            "value": "Batches: 100%"
          }
        },
        "ed9163ea269a40f0adbf2bdd17c2a7e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b98aa29c574088ba22604ad4da9759",
            "max": 142,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a406b18504054d6da3929bfeca4725f2",
            "value": 142
          }
        },
        "23465036a2134481993ae33e4d6c5151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a62024898915400f83193afb9aac0d9a",
            "placeholder": "​",
            "style": "IPY_MODEL_4a4bfe7848364deb9c9c28a7b09bb80d",
            "value": " 142/142 [00:47&lt;00:00,  9.64it/s]"
          }
        },
        "8216505151f64da9b11f2f47c2ded63e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ab3054666449db8a9eafd2c5223ccf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe6bc4bf1b654aea9f9abc41eb645190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49b98aa29c574088ba22604ad4da9759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a406b18504054d6da3929bfeca4725f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a62024898915400f83193afb9aac0d9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a4bfe7848364deb9c9c28a7b09bb80d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}